{"name":"Elasticsearch, Logstash, Kibana (ELK) Docker image","tagline":"Documentation","body":"# Elasticsearch, Logstash, Kibana (ELK) Docker image documentation\r\n\r\nThis web page documents how to use the [sebp/elk](https://hub.docker.com/r/sebp/elk/) Docker image, which provides a convenient centralised log server and log management web interface, by packaging [Elasticsearch](http://www.elasticsearch.org/), [Logstash](http://logstash.net/), and [Kibana](http://www.elasticsearch.org/overview/kibana/), collectively known as ELK.\r\n\r\n### Contents ###\r\n\r\n- [Installation](#installation)\r\n- [Usage](#usage)\r\n\t- [Running the container using Docker Compose](#running-with-docker-compose)\r\n\t- [Creating a dummy log entry](#creating-dummy-log-entry)\r\n- [Forwarding logs](#forwarding-logs)\r\n\t- [Linking a Docker container to the ELK container](#linking-containers)\r\n- [Building the image](#building-image)\r\n- [Extending the image](#extending-image)\r\n\t- [Installing Elasticsearch plugins](#installing-elasticsearch-plugins)\r\n\t- [Installing Logstash plugins](#installing-logstash-plugins)\r\n- [Storing log data](#storing-log-data)\r\n- [Security considerations](#security-considerations)\r\n- [References](#references)\r\n- [About](#about)\r\n\r\n## Installation <a name=\"installation\"></a>\r\n\r\nInstall [Docker](https://docker.com/), either using a native package (Linux) or wrapped in a virtual machine (Windows, OS X – e.g. using [Boot2Docker](http://boot2docker.io/) or [Vagrant](https://www.vagrantup.com/)).\r\n\r\nTo pull this image from the [Docker registry](https://hub.docker.com/r/sebp/elk/), open a shell prompt and enter:\r\n\r\n\t$ sudo docker pull sebp/elk\r\n\r\n**Note** – This image has been built automatically from the source files in the [source Git repository on GitHub](https://github.com/spujadas/elk-docker). If you want to build the image yourself, see the *[Building the image](#building-image)* section below.\r\n\r\n## Usage <a name=\"usage\"></a>\r\n\r\nRun the container from the image with the following command:\r\n\r\n\t$ sudo docker run -p 5601:5601 -p 9200:9200 -p 5000:5000 -it --name elk sebp/elk\r\n\r\nThis command publishes the following ports, which are needed for proper operation of the ELK stack:\r\n\r\n- 5601 (Kibana web interface).\r\n- 9200 (Elasticsearch JSON interface).\r\n- 5000 (Logstash server, receives logs from Logstash forwarders – see the *[Forwarding logs](#forwarding-logs)* section below).\r\n\r\n**Note** – The image also exposes Elasticsearch's transport interface on port 9300. Use the `-p 5300:5300` option with the `docker` command above to publish it.\r\n\r\nThe figure below shows how the pieces fit together.\r\n\r\n![](http://i.imgur.com/RVW12Md.png)\r\n\r\nAccess Kibana's web interface by browsing to `http://<your-host>:5601`, where `<your-host>` is the hostname or IP address of the host Docker is running on (see note), e.g. `localhost` if running a local native version of Docker, or the IP address of the virtual machine if running a VM-hosted version of Docker (see note).\r\n\r\n**Note** – To configure and/or find out the IP address of a VM-hosted Docker installation, see [https://docs.docker.com/installation/windows/](https://docs.docker.com/installation/windows/) (Windows) and [https://docs.docker.com/installation/mac/](https://docs.docker.com/installation/mac/) (OS X) for guidance if using Boot2Docker. If you're using [Vagrant](https://www.vagrantup.com/), you'll need to set up port forwarding (see [https://docs.vagrantup.com/v2/networking/forwarded_ports.html](https://docs.vagrantup.com/v2/networking/forwarded_ports.html).\r\n\r\nYou can stop the container with `^C`, and start it again with `sudo docker start elk`.\r\n\r\nAs from Kibana version 4.0.0, you won't be able to see anything (not even an empty dashboard) until something has been logged (see the *[Creating a dummy log entry](#creating-dummy-log-entry)* sub-section below on how to test your set-up, and the *[Forwarding logs](#forwarding-logs)* section on how to forward logs from regular applications).\r\n\r\n### Running the container using Docker Compose <a name=\"running-with-docker-compose\"></a>\r\n\r\nIf you're using [Docker Compose](https://docs.docker.com/compose/) to manage your Docker services (and if not you really should as it will make your life much easier!), then you can create an entry for the ELK Docker image by adding the following lines to your `docker-compose.yml` file:\r\n\r\n\telk:\r\n\t  image: sebp/elk\r\n\t  ports:\r\n\t    - \"5601:5601\"\r\n\t    - \"9200:9200\"\r\n\t    - \"5000:5000\"\r\n\r\nYou can then start the ELK container like this:\r\n\r\n\t$ sudo docker-compose up elk\r\n\r\n### Creating a dummy log entry <a name=\"creating-dummy-log-entry\"></a>\r\n\r\nIf you haven't got any logs yet and want to manually create a dummy log entry for test purposes (for instance to see the dashboard), first start the container as usual (`sudo docker run ...` or `docker-compose up ...`).\r\n\r\nIn another terminal window, find out the name of the container running ELK, which is displayed in the last column of the output of the `sudo docker ps` command.\r\n\r\n\t$ sudo docker ps\r\n\tCONTAINER ID        IMAGE                  ...   NAMES\r\n\t86aea21cab85        elkdocker_elk:latest   ...   elkdocker_elk_1\r\n\r\nOpen a shell prompt in the container and type (replacing `<container-name>` with the name of the container, e.g. `elkdocker_elk_1` in the example above):\r\n\r\n\t$ sudo docker exec -it <container-name> /bin/bash\r\n\r\nAt the prompt, enter:\r\n\r\n\t# /opt/logstash/bin/logstash -e 'input { stdin { } } output { elasticsearch { host => localhost } }'\r\n\r\nWait for Logstash to start (as indicated by the message `Logstash startup completed`), then type some dummy text followed by Enter to create a log entry:\r\n\r\n\tthis is a dummy entry\r\n\r\n**Note** - You can create as many entries as you want. Use `^C` to go back to the bash prompt.\r\n\r\nIf you browse to `http://<your-host>:9200/_search?pretty` (e.g. [http://localhost:9200/_search?pretty](http://localhost:9200/_search?pretty) for a local native instance of Docker) you'll see that Elasticsearch has indexed the entry:\r\n\r\n\t{\r\n\t  ...\r\n\t  \"hits\": {\r\n\t    ...\r\n\t    \"hits\": [ {\r\n\t      \"_index\": \"logstash-...\",\r\n\t      \"_type\": \"logs\",\r\n\t      ...\r\n\t      \"_source\": { \"message\": \"this is a dummy entry\", \"@version\": \"1\", \"@timestamp\": ... }\r\n\t    } ]\r\n\t  }\r\n\t}\r\n\r\nYou can now browse to Kibana's web interface at `http://<your-host>:5601` (e.g. [http://localhost:5601](http://localhost:5601) for a local native instance of Docker).\r\n\r\nMake sure that the drop-down \"Time-field name\" field is pre-populated with the value `@timestamp`, then click on \"Create\", and you're good to go.\r\n\r\n## Forwarding logs <a name=\"forwarding-logs\"></a>\r\n\r\nForwarding logs from a host relies on a Logstash forwarder agent that collects logs (e.g. from log files, from the syslog daemon) and sends them to our instance of Logstash.\r\n\r\nInstall [Logstash forwarder](https://github.com/elasticsearch/logstash-forwarder) on the host you want to collect and forward logs from (see the *[References](#references)* section below for links to detailed instructions).\r\n\r\nHere is a sample configuration file for Logstash forwarder, that forwards syslog and authentication logs, as well as [nginx](http://nginx.org/) logs.\r\n\r\n\t{\r\n\t  \"network\": {\r\n\t    \"servers\": [ \"elk:5000\" ],\r\n\t    \"timeout\": 15,\r\n\t    \"ssl ca\": \"/etc/pki/tls/certs/logstash-forwarder.crt\"\r\n\t  },\r\n\t  \"files\": [\r\n\t    {\r\n\t      \"paths\": [\r\n\t        \"/var/log/syslog\",\r\n\t        \"/var/log/auth.log\"\r\n\t       ],\r\n\t      \"fields\": { \"type\": \"syslog\" }\r\n\t    },\r\n\t    {\r\n\t      \"paths\": [\r\n\t        \"/var/log/nginx/access.log\"\r\n\t       ],\r\n\t      \"fields\": { \"type\": \"nginx-access\" }\r\n\t    }\r\n\t   ]\r\n\t}\r\n\r\nBy default (see `/etc/init.d/logstash-forwarder` if you need to tweak anything):\r\n\r\n- The Logstash forwarder configuration file must be located in `/etc/logstash-forwarder`.\r\n- The Logstash forwarder needs a syslog daemon (e.g. rsyslogd, syslog-ng) to be running.\r\n\r\nIn the sample configuration file, make sure that you:\r\n\r\n- Replace `elk` in `elk:5000` with the hostname or IP address of the ELK-serving host.\r\n- Copy the `logstash-forwarder.crt` file (which contains the Logstash server's certificate) from the ELK image to `/etc/pki/tls/certs/logstash-forwarder.crt`.\r\n\r\n**Note** – The ELK image includes configuration items (`/etc/logstash/conf.d/11-nginx.conf` and `/opt/logstash/patterns/nginx`) to parse nginx access logs, as forwarded by the Logstash forwarder instance above.\r\n\r\n### Linking a Docker container to the ELK container <a name=\"linking-containers\"></a>\r\n\r\nIf you want to forward logs from a Docker container to the ELK container, then you need to link the two containers.\r\n\r\n**Note** – The log-emitting Docker container must have a Logstash forwarder agent running in it for this to work.\r\n\r\nFirst of all, give the ELK container a name (e.g. `elk`) using the `--name` option:\r\n\r\n\t$ sudo docker run -p 5601:5601 -p 9200:9200 -p 5000:5000 -it --name elk sebp/elk\r\n\r\nThen start the log-emitting container with the `--link` option (replacing `your/image` with the name of the Logstash-forwarder-enabled image you're forwarding logs from):\r\n\r\n\t$ sudo docker run -p 80:80 -it --link elk:elk your/image\r\n\r\nFrom the perspective of the log emitting container, the ELK container is now known as `elk`, which is the hostname to be used in the `logstash-forwarder` configuration file.\r\n\r\nWith Compose here's what example entries for a (locally built log-generating) container and an ELK container might look like in the `docker-compose.yml` file.\r\n\r\n\tyourapp:\r\n\t  image: your/image\r\n\t  ports:\r\n\t    - \"80:80\"\r\n\t  links:\r\n\t    - elk\r\n\r\n\telk:\r\n\t  image: sebp/elk\r\n\t  ports:\r\n\t    - \"5601:5601\"\r\n\t    - \"9200:9200\"\r\n\t    - \"5000:5000\"\r\n\r\n## Building the image <a name=\"building-image\"></a>\r\n\r\nTo build the Docker image from the source files, first clone the [Git repository](https://github.com/spujadas/elk-docker), go to the root of the cloned directory (i.e. the directory that contains `Dockerfile`), and:\r\n\r\n- If you're using the vanilla `docker` command then run `sudo docker build -t <repository-name> .`, where `<repository-name>` is the repository name to be applied to the image, which you can then use to run the image with the `docker run` command.\r\n\r\n- If you're using Compose then run `sudo docker-compose build elk`, which uses the `docker-compose.yml` file from the source repository to build the image. You can then run the built image with `sudo docker-compose up`.\r\n\r\n\r\n## Extending the image <a name=\"extending-image\"></a>\r\n\r\nTo extend the image, you can either fork the source Git repository and hack away, or – more in the spirit of the Docker philosophy – use the image as a base image and build on it, adding files (e.g. configuration files to process logs sent by log-producing applications, plugins for Elasticsearch) and overwriting files (e.g. configuration files, certificate and private key files) as required.\r\n\r\nTo create a new image based on this base image, you want your `Dockerfile` to include:\r\n\r\n\tFROM sebp/elk\r\n\r\nfollowed by instructions to extend the image (see Docker's [Dockerfile Reference page](https://docs.docker.com/reference/builder/) for more information).\r\n\r\nThe next few subsections present some typical use cases.\r\n\r\n### Installing Elasticsearch plugins <a name=\"installing-elasticsearch-plugins\"></a>\r\n\r\nElasticsearch's home directory in the image is `/usr/share/elasticsearch`, its [plugin management script](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-plugins.html) (`plugin`) resides in the `bin` subdirectory, and plugins are installed in `plugins`.\r\n\r\nA `Dockerfile` like the following will extend the base image and install Elastic HQ, a management and monitoring plugin for Elasticsearch, using `plugin`.\r\n\r\n\tFROM sebp/elk\r\n\r\n\tENV ES_HOME /usr/share/elasticsearch\r\n\tWORKDIR ${ES_HOME}\r\n\r\n\tRUN bin/plugin -i royrusso/elasticsearch-HQ\r\n\r\nYou can now build the new image (see the *[Building the image](#building-image)* section above) and run the container in the same way as you did with the base image. The Elastic HQ interface will be accessible at `http://<your-host>:9200/_plugin/HQ/` (e.g. [http://localhost:9200/_plugin/HQ/](http://localhost:9200/_plugin/HQ/) for a local native instance of Docker).\r\n\r\n### Installing Logstash plugins <a name=\"installing-logstash-plugins\"></a>\r\n\r\nThe name of Logstash's home directory in the image is stored in the `LOGSTASH_HOME` environment variable (which is set to `/opt/logstash` in the base image). Logstash's plugin management script (`plugin`) is located in the `bin` subdirectory.\r\n\r\nThe following `Dockerfile` can be used to extend the base image and install the [RSS input plugin](https://www.elastic.co/guide/en/logstash/current/plugins-inputs-rss.html):\r\n\r\n\tFROM sebp/elk\r\n\r\n\tWORKDIR ${LOGSTASH_HOME}\r\n\tRUN bin/plugin install logstash-input-rss\r\n\r\nSee the *[Building the image](#building-image)* section above for instructions on building the new image. You can then run a container based on this image using the same command line as the one in the *[Usage](#usage)* section.\r\n\r\n## Storing log data <a name=\"storing-log-data\"></a>\r\n\r\nIn order to keep log data across container restarts, this image mounts `/var/lib/elasticsearch` — which is the directory that Elasticsearch stores its data in — as a volume.\r\n\r\nYou may however want to use a dedicated data volume to store this log data, for instance to facilitate back-up and restore operations.\r\n\r\nOne way to do this with the `docker` command-line tool is to first create a named container called `elk_data` with a bound Docker volume by using the `-v` option:\r\n\r\n\t$ sudo docker run -p 5601:5601 -p 9200:9200 -5000:5000 -v /var/lib/elasticsearch --name elk_data sebp/elk\r\n\r\nYou can now reuse the persistent volume from that container using the `--volumes-from` option:\r\n\r\n\t$ sudo docker run -p 5601:5601 -p 9200:9200 -p 5000:5000 --volumes-from elk_data --name elk sebp/elk\r\n\r\n**Note** – By design, Docker never deletes a volume automatically (e.g. when no longer used by any container). Whilst this avoids accidental data loss, it also means that things can become messy if you're not managing your volumes properly (i.e. using the `-v` option when removing containers with `docker rm` to also delete the volumes... bearing in mind that the actual volume won't be deleted as long as at least one container is still referencing it, even if it's not running). As of this writing, managing Docker volumes can be a bit of a headache, so you might want to have a look at [docker-cleanup-volumes](https://github.com/chadoe/docker-cleanup-volumes), a shell script that deletes unused Docker volumes.\r\n\r\nSee Docker's page on [Managing Data in Containers](https://docs.docker.com/userguide/dockervolumes/) and Container42's [Docker In-depth: Volumes](http://container42.com/2014/11/03/docker-indepth-volumes/) page for more information on managing data volumes.\r\n\r\n## Security considerations <a name=\"security-considerations\"></a>\r\n\r\nAs it stands this image is meant for local test use, and as such hasn't been secured: access to the ELK services is not restricted, and a default authentication server certificate (`logstash-forwarder.crt`) and private key (`logstash-forwarder.key`) are bundled with the image.\r\n\r\nTo harden this image, at the very least you would want to:\r\n\r\n- Restrict the access to the ELK services to authorised hosts/networks only, as described in e.g. [Elasticsearch Scripting and Security](http://www.elasticsearch.org/blog/scripting-security/) and [Elastic Security: Deploying Logstash, ElasticSearch, Kibana \"securely\" on the Internet](http://blog.eslimasec.com/2014/05/elastic-security-deploying-logstash.html).\r\n- Password-protect the access to Kibana and Elasticsearch (see [SSL And Password Protection for Kibana](http://technosophos.com/2014/03/19/ssl-password-protection-for-kibana.html)).\r\n- Generate a new self-signed authentication certificate for the Logstash server (`cd /etc/pki/tls; sudo openssl req -x509 -batch -nodes -days 3650 -newkey rsa:2048 -keyout private/logstash-forwarder.key -out certs/logstash-forwarder.crt` for a 10-year certificate) or (better) get a proper certificate from a commercial provider (known as a certificate authority), and keep the private key private.\r\n\r\n## References <a name=\"references\"></a>\r\n\r\n- [How To Install Elasticsearch, Logstash, and Kibana 4 on Ubuntu 14.04](https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-4-on-ubuntu-14-04)\r\n- [The Docker Book](http://www.dockerbook.com/)\r\n- [The Logstash Book](http://www.logstashbook.com/)\r\n- [Elastic's reference documentation](https://www.elastic.co/guide/index.html)\r\n\r\n## About <a name=\"about\"></a>\r\n\r\nWritten by [Sébastien Pujadas](https://pujadas.net), released under the [Apache 2 license](https://www.apache.org/licenses/LICENSE-2.0).\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}